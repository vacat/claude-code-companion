#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Token Usage Statistics Analyzer

ç‹¬ç«‹çš„ç»Ÿè®¡ç¨‹åºï¼Œåˆ†æç‰¹å®šæ—¶é—´æ®µå†…çš„ API è¯·æ±‚æ•°æ®ã€‚
ä» SQLite æ•°æ®åº“ä¸­æå–å’Œåˆ†æ token ä½¿ç”¨æƒ…å†µã€rate limit çŠ¶æ€å’ŒGACç§¯åˆ†ã€‚

ä½œè€…ï¼šClaude Code Companion
ç‰ˆæœ¬ï¼š1.1.0
"""

import sqlite3
import json
import argparse
from datetime import datetime
from typing import Dict, Tuple, Any, Optional
import sys
from collections import defaultdict


class TokenUsageAnalyzer:
    """Token ä½¿ç”¨æƒ…å†µåˆ†æå™¨"""
    
    def __init__(self, db_path: str, debug: bool = False):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            db_path: SQLite æ•°æ®åº“è·¯å¾„
            debug: æ˜¯å¦å¯ç”¨è°ƒè¯•æ¨¡å¼
        """
        self.db_path = db_path
        self.debug = debug
        self.conn = None
        self.stats = defaultdict(lambda: {
            'request_count': 0,
            'input_tokens': 0,
            'cache_creation_input_tokens': 0,
            'cache_read_input_tokens': 0,
            'output_tokens': 0,
            'unique_sessions': set()  # ç”¨äºè·Ÿè¸ªæ¯ä¸ªçŠ¶æ€çš„unique session
        })
        
    def connect_database(self) -> bool:
        """
        è¿æ¥åˆ° SQLite æ•°æ®åº“
        
        Returns:
            bool: è¿æ¥æˆåŠŸè¿”å› Trueï¼Œå¤±è´¥è¿”å› False
        """
        try:
            self.conn = sqlite3.connect(self.db_path)
            self.conn.row_factory = sqlite3.Row  # å…è®¸æŒ‰åˆ—åè®¿é—®
            print(f"âœ“ æˆåŠŸè¿æ¥åˆ°æ•°æ®åº“: {self.db_path}")
            return True
        except sqlite3.Error as e:
            print(f"âœ— æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            return False
    
    def close_database(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self.conn:
            self.conn.close()
            
    def build_query(self, start_time: str, end_time: str) -> str:
        """
        æ„å»º SQL æŸ¥è¯¢è¯­å¥
        
        Args:
            start_time: å¼€å§‹æ—¶é—´ (UTCæ ¼å¼: YYYY-MM-DD HH:MM:SS)
            end_time: ç»“æŸæ—¶é—´ (UTCæ ¼å¼: YYYY-MM-DD HH:MM:SS)
            
        Returns:
            str: SQL æŸ¥è¯¢è¯­å¥
        """
        query = """
        SELECT 
            original_response_body,
            original_response_headers,
            model,
            timestamp,
            endpoint,
            status_code,
            session_id
        FROM request_logs 
        WHERE timestamp >= ?
          AND timestamp <= ?
          AND endpoint LIKE '%api.anthropic.com%'
          AND status_code = 200
          AND (model IS NULL OR model NOT LIKE '%haiku%')
        ORDER BY timestamp
        """
        return query
        
    def parse_response_body(self, body: str) -> Dict[str, int]:
        """
        è§£æå“åº”ä½“ä¸­çš„ token ä½¿ç”¨æƒ…å†µ (å¤„ç† stream æ ¼å¼)
        
        Args:
            body: å“åº”ä½“ stream æ ¼å¼å­—ç¬¦ä¸²
            
        Returns:
            Dict: åŒ…å«å„ç§ token æ•°é‡çš„å­—å…¸
        """
        token_usage = {
            'input_tokens': 0,
            'cache_creation_input_tokens': 0,
            'cache_read_input_tokens': 0,
            'output_tokens': 0
        }
        
        try:
            if not body or body.strip() == '':
                if self.debug:
                    print("DEBUG: å“åº”ä½“ä¸ºç©º")
                return token_usage
            
            # å¤„ç† stream æ ¼å¼å“åº”
            lines = body.strip().split('\n')
            
            if self.debug:
                print(f"DEBUG: æ‰¾åˆ° {len(lines)} è¡Œå“åº”æ•°æ®")
            
            for line_num, line in enumerate(lines, 1):
                line = line.strip()
                
                # è·³è¿‡ç©ºè¡Œå’Œédataè¡Œ
                if not line:
                    continue
                if not line.startswith('data: '):
                    if self.debug and line:
                        print(f"ğŸ” DEBUG: ç¬¬{line_num}è¡Œè·³è¿‡édataè¡Œ: {line[:50]}...")
                    continue
                    
                # æå–JSONéƒ¨åˆ†
                json_str = line[6:]  # å»æ‰ "data: " å‰ç¼€
                
                # è·³è¿‡ç‰¹æ®Šæ ‡è®°
                if json_str in ['[DONE]', '']:
                    if self.debug:
                        print(f"ğŸ” DEBUG: ç¬¬{line_num}è¡Œè·³è¿‡ç‰¹æ®Šæ ‡è®°: {json_str}")
                    continue
                
                try:
                    data = json.loads(json_str)
                    event_type = data.get('type', 'unknown')
                    
                    if self.debug:
                        print(f"ğŸ” DEBUG: ç¬¬{line_num}è¡Œè§£ææˆåŠŸï¼Œäº‹ä»¶ç±»å‹: {event_type}")
                    
                    # æ£€æŸ¥message_startäº‹ä»¶ä¸­çš„usageï¼ˆåˆå§‹å€¼ï¼‰
                    if event_type == 'message_start':
                        message = data.get('message', {})
                        usage = message.get('usage', {})
                        
                        if usage and self.debug:
                            print(f"ğŸ” DEBUG: message_start usage: {usage}")
                        
                        # ä½¿ç”¨message_startçš„tokenå€¼ä½œä¸ºåˆå§‹å€¼
                        for key in token_usage.keys():
                            if key in usage:
                                token_usage[key] = usage.get(key, 0)
                    
                    # æ£€æŸ¥message_deltaäº‹ä»¶ä¸­çš„usageï¼ˆæœ€ç»ˆå®Œæ•´ç»Ÿè®¡ï¼‰
                    elif event_type == 'message_delta':
                        # message_deltaå¯èƒ½åœ¨deltaä¸­åŒ…å«usageï¼Œä¹Ÿå¯èƒ½ç›´æ¥åœ¨æ ¹çº§åˆ«åŒ…å«usage
                        usage = None
                        if 'delta' in data and 'usage' in data['delta']:
                            usage = data['delta']['usage']
                        elif 'usage' in data:
                            usage = data['usage']
                        
                        if usage and self.debug:
                            print(f"ğŸ” DEBUG: message_delta usage: {usage}")
                        
                        # message_deltaçš„usageåŒ…å«å®Œæ•´çš„æœ€ç»ˆç»Ÿè®¡ï¼Œè¦†ç›–æ‰€æœ‰tokenå€¼
                        if usage:
                            for key in token_usage.keys():
                                if key in usage:
                                    token_usage[key] = usage.get(key, 0)
                    
                    # æ£€æŸ¥ç›´æ¥åŒ…å«usageå­—æ®µçš„å…¶ä»–äº‹ä»¶
                    elif 'usage' in data:
                        usage = data['usage']
                        if self.debug:
                            print(f"ğŸ” DEBUG: ç›´æ¥usageå­—æ®µ (äº‹ä»¶ç±»å‹: {event_type}): {usage}")
                        # ä½¿ç”¨æœ€åå‡ºç°çš„tokenå€¼ï¼ˆè¦†ç›–ï¼Œä¸ç´¯åŠ ï¼‰
                        for key in token_usage.keys():
                            if key in usage:
                                token_usage[key] = usage.get(key, 0)
                            
                except json.JSONDecodeError as e:
                    # å•è¡ŒJSONè§£æå¤±è´¥ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€è¡Œ
                    if self.debug:
                        print(f"ğŸ” DEBUG: ç¬¬{line_num}è¡ŒJSONè§£æå¤±è´¥: {e}")
                        print(f"ğŸ” DEBUG: åŸå§‹å†…å®¹: {json_str[:100]}...")
                    continue
                    
        except Exception as e:
            print(f"âš  å“åº”ä½“è§£æé”™è¯¯: {e}")
            
        if self.debug:
            print(f"ğŸ” DEBUG: æœ€ç»ˆæå–çš„tokenæ•°é‡: {token_usage}")
            
        return token_usage
        
    def parse_response_headers(self, headers: str) -> str:
        """
        è§£æå“åº”å¤´ä¸­çš„ rate limit çŠ¶æ€
        
        Args:
            headers: å“åº”å¤´ JSON å­—ç¬¦ä¸²
            
        Returns:
            str: Rate limit çŠ¶æ€ (allowed/allowed_warning/rejected/unknown)
        """
        try:
            if not headers or headers.strip() == '':
                return 'unknown'
                
            data = json.loads(headers)
            status = data.get('Anthropic-Ratelimit-Unified-5h-Status', 'unknown')
            return status.lower()
            
        except json.JSONDecodeError as e:
            print(f"âš  JSON è§£æå¤±è´¥ (response_headers): {e}")
            return 'unknown'
        except Exception as e:
            print(f"âš  å“åº”å¤´è§£æé”™è¯¯: {e}")
            return 'unknown'
            
    def analyze_data(self, start_time: str, end_time: str) -> Tuple[int, int]:
        """
        åˆ†ææŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„æ•°æ®
        
        Args:
            start_time: å¼€å§‹æ—¶é—´ (UTC)
            end_time: ç»“æŸæ—¶é—´ (UTC)
            
        Returns:
            Tuple[int, int]: (å¤„ç†çš„è®°å½•æ€»æ•°, è§£æé”™è¯¯çš„è®°å½•æ•°)
        """
        if not self.conn:
            print("âœ— æ•°æ®åº“æœªè¿æ¥")
            return 0, 0
            
        query = self.build_query(start_time, end_time)
        cursor = self.conn.cursor()
        
        try:
            cursor.execute(query, (start_time, end_time))
            rows = cursor.fetchall()
            
            if not rows:
                print("âš  æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ•°æ®")
                return 0, 0
                
            total_records = len(rows)
            parse_errors = 0
            
            print(f"ğŸ“Š å¼€å§‹åˆ†æ {total_records} æ¡è®°å½•...")
            
            for i, row in enumerate(rows, 1):
                if self.debug:
                    print(f"\nğŸ” DEBUG: === å¤„ç†ç¬¬ {i}/{total_records} æ¡è®°å½• ===")
                    print(f"ğŸ” DEBUG: æ—¶é—´æˆ³: {row['timestamp']}")
                    print(f"ğŸ” DEBUG: ç«¯ç‚¹: {row['endpoint']}")
                    print(f"ğŸ” DEBUG: æ¨¡å‹: {row['model']}")
                    print(f"ğŸ” DEBUG: Session ID: {row['session_id']}")
                
                # è§£æ token ä½¿ç”¨æƒ…å†µ
                token_usage = self.parse_response_body(row['original_response_body'])
                
                # è§£æ rate limit çŠ¶æ€
                rate_limit_status = self.parse_response_headers(row['original_response_headers'])
                
                if self.debug:
                    print(f"ğŸ” DEBUG: Rate limit çŠ¶æ€: {rate_limit_status}")
                    print(f"ğŸ” DEBUG: Token ä½¿ç”¨æƒ…å†µ: {token_usage}")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰è§£æé”™è¯¯
                if rate_limit_status == 'unknown':
                    parse_errors += 1
                
                # ç´¯è®¡ç»Ÿè®¡
                status_stats = self.stats[rate_limit_status]
                status_stats['request_count'] += 1
                for key, value in token_usage.items():
                    status_stats[key] += value
                
                # æ·»åŠ session_idåˆ°å¯¹åº”çŠ¶æ€çš„unique sessioné›†åˆä¸­
                session_id = row['session_id']
                if session_id:  # åªæœ‰å½“session_idä¸ä¸ºç©ºæ—¶æ‰æ·»åŠ 
                    status_stats['unique_sessions'].add(session_id)
                    
            return total_records, parse_errors
            
        except sqlite3.Error as e:
            print(f"âœ— SQL æŸ¥è¯¢é”™è¯¯: {e}")
            return 0, 0
        finally:
            cursor.close()
            
    def format_number(self, num: int) -> str:
        """æ ¼å¼åŒ–æ•°å­—ï¼Œæ·»åŠ åƒåˆ†ä½åˆ†éš”ç¬¦"""
        return f"{num:,}"
        
    def calculate_gac_points(self, stats: Dict) -> int:
        """
        è®¡ç®—GACç§¯åˆ†
        
        å…¬å¼: round((æ€»tokenæ•° / 3072)) + (æ€»è¯·æ±‚æ•° * 2)
        æ€»tokenæ•° = input_tokens + cache_creation_input_tokens + cache_read_input_tokens + output_tokens
        
        Args:
            stats: åŒ…å«tokenç»Ÿè®¡çš„å­—å…¸
            
        Returns:
            int: GACç§¯åˆ†
        """
        total_tokens = (
            stats['input_tokens'] + 
            stats['cache_creation_input_tokens'] + 
            stats['cache_read_input_tokens'] + 
            stats['output_tokens']
        )
        
        token_points = round(total_tokens / 3072)
        request_points = stats['request_count'] * 2
        
        return token_points + request_points
        
    def print_results(self, start_time: str, end_time: str, 
                     total_records: int, parse_errors: int):
        """
        æ‰“å°ç»Ÿè®¡ç»“æœ
        
        Args:
            start_time: GMT+8 å¼€å§‹æ—¶é—´
            end_time: GMT+8 ç»“æŸæ—¶é—´  
            total_records: å¤„ç†çš„è®°å½•æ€»æ•°
            parse_errors: è§£æé”™è¯¯æ•°
        """
        print("\n" + "="*50)
        print("Token Usage Statistics Report")
        print("="*50)
        print(f"Time Range: {start_time} - {end_time} (GMT+8)")
        print("Filter: api.anthropic.com, Status=200, Non-Haiku models")
        print("\nSummary by Rate Limit Status:")
        print("-" * 30)
        
        # æŒ‰çŠ¶æ€ä¼˜å…ˆçº§æ’åº
        status_order = ['allowed', 'allowed_warning', 'rejected', 'unknown']
        
        for status in status_order:
            if status in self.stats:
                stats = self.stats[status]
                status_display = status.upper().replace('_', '_')
                
                print(f"\n{status_display}:")
                print(f"  Request Count: {self.format_number(stats['request_count'])}")
                print(f"  Unique Sessions: {self.format_number(len(stats['unique_sessions']))}")
                
                if stats['request_count'] > 0:
                    print(f"  Total Input Tokens: {self.format_number(stats['input_tokens'])}")
                    if stats['cache_creation_input_tokens'] > 0:
                        print(f"  Total Cache Creation Tokens: {self.format_number(stats['cache_creation_input_tokens'])}")
                    if stats['cache_read_input_tokens'] > 0:
                        print(f"  Total Cache Read Tokens: {self.format_number(stats['cache_read_input_tokens'])}")
                    print(f"  Total Output Tokens: {self.format_number(stats['output_tokens'])}")
                    
                    # è®¡ç®—å¹¶æ˜¾ç¤ºGACç§¯åˆ†
                    gac_points = self.calculate_gac_points(stats)
                    print(f"  GAC Points: {self.format_number(gac_points)}")
        
        # æ˜¾ç¤ºå…¶ä»–çŠ¶æ€ï¼ˆå¦‚æœæœ‰ï¼‰
        other_statuses = set(self.stats.keys()) - set(status_order)
        for status in sorted(other_statuses):
            stats = self.stats[status]
            print(f"\n{status.upper()}:")
            print(f"  Request Count: {self.format_number(stats['request_count'])}")
            print(f"  Unique Sessions: {self.format_number(len(stats['unique_sessions']))}")
            
            if stats['request_count'] > 0:
                # è®¡ç®—å¹¶æ˜¾ç¤ºGACç§¯åˆ†
                gac_points = self.calculate_gac_points(stats)
                print(f"  GAC Points: {self.format_number(gac_points)}")
            
        print(f"\nTotal Processed Records: {self.format_number(total_records)}")
        if parse_errors > 0:
            print(f"Parse Errors: {self.format_number(parse_errors)}")
            
    def validate_time_format(self, time_str: str) -> bool:
        """
        éªŒè¯æ—¶é—´æ ¼å¼
        
        Args:
            time_str: æ—¶é—´å­—ç¬¦ä¸² (YYYY-MM-DD HH:MM:SS)
            
        Returns:
            bool: æ ¼å¼æ­£ç¡®è¿”å› True
        """
        try:
            datetime.strptime(time_str, '%Y-%m-%d %H:%M:%S')
            return True
        except ValueError as e:
            print(f"âœ— æ—¶é—´æ ¼å¼é”™è¯¯: {e}")
            return False
            
    def run_analysis(self, start_time: str, end_time: str) -> bool:
        """
        è¿è¡Œå®Œæ•´çš„åˆ†ææµç¨‹
        
        Args:
            start_time: å¼€å§‹æ—¶é—´ (GMT+8æ ¼å¼: YYYY-MM-DD HH:MM:SS)
            end_time: ç»“æŸæ—¶é—´ (GMT+8æ ¼å¼: YYYY-MM-DD HH:MM:SS)
            
        Returns:
            bool: åˆ†ææˆåŠŸè¿”å› True
        """
        print(f"ğŸ” å¼€å§‹åˆ†ææ—¶é—´èŒƒå›´: {start_time} - {end_time} (GMT+8)")
        
        # éªŒè¯æ—¶é—´æ ¼å¼
        if not self.validate_time_format(start_time) or not self.validate_time_format(end_time):
            return False
        
        # è¿æ¥æ•°æ®åº“
        if not self.connect_database():
            return False
            
        try:
            # ç›´æ¥ä½¿ç”¨GMT+8æ—¶é—´è¿›è¡Œåˆ†æï¼ˆä¸è¿›è¡Œæ—¶åŒºè½¬æ¢ï¼‰
            total_records, parse_errors = self.analyze_data(start_time, end_time)
            
            if total_records == 0:
                print("âš  æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ•°æ®è®°å½•")
                return False
                
            # æ‰“å°ç»“æœ
            self.print_results(start_time, end_time, total_records, parse_errors)
            
            return True
            
        finally:
            self.close_database()


def main():
    """ä¸»ç¨‹åºå…¥å£"""
    parser = argparse.ArgumentParser(
        description='Token Usage Statistics Analyzer',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
ç¤ºä¾‹ç”¨æ³•:
  %(prog)s --db ./logs/logs.db
  %(prog)s --db /path/to/logs.db --start "2025-08-26 10:00:00" --end "2025-08-26 20:00:00"
  %(prog)s --db ./logs/logs.db --debug  # å¯ç”¨è°ƒè¯•æ¨¡å¼
  %(prog)s --help
        '''
    )
    
    parser.add_argument(
        '--db', 
        default='./logs/logs.db',
        help='SQLite æ•°æ®åº“æ–‡ä»¶è·¯å¾„ (é»˜è®¤: ./logs/logs.db)'
    )
    
    parser.add_argument(
        '--start',
        default='2025-08-26 14:00:00',
        help='å¼€å§‹æ—¶é—´ GMT+8 (æ ¼å¼: YYYY-MM-DD HH:MM:SS, é»˜è®¤: 2025-08-26 14:00:00)'
    )
    
    parser.add_argument(
        '--end',
        default='2025-08-26 18:00:00', 
        help='ç»“æŸæ—¶é—´ GMT+8 (æ ¼å¼: YYYY-MM-DD HH:MM:SS, é»˜è®¤: 2025-08-26 18:00:00)'
    )
    
    parser.add_argument(
        '--debug',
        action='store_true',
        help='å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼Œæ˜¾ç¤ºè¯¦ç»†çš„è§£æè¿‡ç¨‹'
    )
    
    args = parser.parse_args()
    
    print("Token Usage Statistics Analyzer v1.1.0")
    print("-" * 40)
    
    if args.debug:
        print("ğŸ” è°ƒè¯•æ¨¡å¼å·²å¯ç”¨")
    
    # åˆ›å»ºåˆ†æå™¨å®ä¾‹
    analyzer = TokenUsageAnalyzer(args.db, debug=args.debug)
    
    # è¿è¡Œåˆ†æ
    success = analyzer.run_analysis(args.start, args.end)
    
    if not success:
        sys.exit(1)
        
    print("\nâœ“ åˆ†æå®Œæˆ!")


if __name__ == '__main__':
    main()